
# Kubernetes Cluster Provision
The repository contains the required tools to provision and deploy a test Kubernetes cluster. 
*Note that the current workflow is not suitable for be used to setup cluster for production environments.* Respectively, each phase follow the workflow:

### 1. Machine Provisioning & Configuration
The VM provisioning is performed by **Vagrant**, using as provider **Virtualbox** for an easier, local testing environment. The variables defined in the [Vagrantfile](Vagrantfile) allows customization of regarding the OS base image (`ubuntu/jammy64`), worker nodes IP addresses, etc.

**Ansible** is integrated by using the [Ansible Provider](https://developer.hashicorp.com/vagrant/docs/provisioning/ansible), allowing to execute the three main [playbooks](./playbooks/) used to configure the provisioned VMs and deploy the Kubernetes components.
The three key playbooks are:
- `general.yml`: Ensure that the required packages are available, configures the Docker and Kubernetes APT repository, set the required `overlay` and `br_netfilter` (*see [References \& sources](#sources)*), install Kubernetes binaries and restart the services.
- `master.yml`: Initialize the cluster, specifying master node IP address, CIDR pod network and secondary flags to avoid minor errors due to constrained resources, install Calico operator and custom resources definition for Pod Networking, copy cluster configuration file to enable `kubectl` for root user. Finally, save locally under `playbooks/output/` folder the `admin.conf` and `kubeadm-join.sh`. 
- `worker.yml`: Retrieve the `kubeadm-join.sh` script generated by the `master` node to join the cluster and execute it.

The local execution make easier the Kubernetes Provisioning and Management, acting as a glue to transfer configurations and parameters produced by the Ansible execution and exported locally under the [playbooks/output/](./playbooks/output/) folder.


### 2. Kubernetes Provisioning & Management
**Terraform** is used to perform the following list of management activity, given the cluster configuration (`admin.conf`) retrieved locally under [playbooks/output/](./playbooks/output/):
1. *Create `kiratech-tech` namespace*: all the further activities are performed inside this namespace
2. *Create cluster role and cluster role binding*: the role grants the "default" service account in the `kiratech-test` namespace read-only access to various Kubernetes resources like nodes, pods, deployments, and network configurations. The binding bind to default account in the same namespace.
3. *Execute **Kubescape***: scan the cluster for potential security vulnerabilities. Kubescape has been choosen as it can test against various security frameworks, including NSA, MITRE and SOC2, integrates also Runtime Threat Detection, it's provided as Operator, perform YAML and Helm validation and it's well integrated with a broad range of tools like Prometheus, most of CI/CD pipelines and IDE currently used. Extensive information on how Hardening Kubernetes with Kubescape can be found [here](https://kubescape.io/docs/guides/kubescape-cli/#workload-scanning).
4. *Deploy local Helm Chart*: use Terraform resource `helm_release` to deploy a [microservice application demo](https://github.com/GoogleCloudPlatform/microservices-demo) composed by 11 services. The web interface is reachable on each worker node address on port `30927`. This choice is explained in the following section as minor modifications has been made to the Helm Chart to allows the deploy on the local Kubernetes cluster.

## Usage
The specified version are the one under which the workflow has been tested:
- *Ansible - v2.17.3*
- *Vagrant - v2.4.1*
- *VirtualBox - v7.0.16*
- *Terraform - v1.9.5*

To execute the workflow:
```bash
vagrant up --provision
```
It will take a couple of minutes to download the corresponding OS image (*Vagrant boxes*). It's recommended, in case of few resources locally to use a lighter ubuntu based box by looking at [Hashicorp - Discover Vagrant Boxes](https://portal.cloud.hashicorp.com/vagrant/discover).
Automatically it will execute the describe provisioning action. 
To check the machines status:
```bash
vagrant global-status
```

After the `master` machine is ready, it's possible to SSH by:
```bash
vagrant ssh-config > k8s-vagrant-nodes
ssh -F k8s-vagrant-nodes master
```
After the workers are up and running, it's possible to re-execute `vagrant ssh-config` to log inside the worker machines.

After all the workers have finished executing the playbooks, joining the cluster, type:
```bash
terraform plan
terraform apply
```
to execute the Kubernetes Cluster Provision. 


To cleanup:
```bash
terraform destroy
vagrant destroy
```

## Structure
```bash
â”œâ”€â”€ helm-chart  # Microservices App Helm Chart folder
â”‚   â”œâ”€â”€ Chart.yaml
â”‚   â”œâ”€â”€ templates
â”‚   â”‚   â”œâ”€â”€ adservice.yaml
â”‚   â”‚   â”œâ”€â”€ cartservice.yaml
â”‚   â”‚   â”œâ”€â”€ checkoutservice.yaml
â”‚   â”‚   â”œâ”€â”€ common.yaml
â”‚   â”‚   â”œâ”€â”€ currencyservice.yaml
â”‚   â”‚   â”œâ”€â”€ emailservice.yaml
â”‚   â”‚   â”œâ”€â”€ frontend.yaml
â”‚   â”‚   â”œâ”€â”€ loadgenerator.yaml
â”‚   â”‚   â”œâ”€â”€ opentelemetry-collector.yaml
â”‚   â”‚   â”œâ”€â”€ paymentservice.yaml
â”‚   â”‚   â”œâ”€â”€ productcatalogservice.yaml
â”‚   â”‚   â”œâ”€â”€ recommendationservice.yaml
â”‚   â”‚   â”œâ”€â”€ redis.yaml
â”‚   â”‚   â””â”€â”€ shippingservice.yaml
â”‚   â””â”€â”€ values.yaml
â”œâ”€â”€ playbooks # Ansible Playbooks Folder
â”‚   â”œâ”€â”€ config
â”‚   â”‚   â””â”€â”€ config.toml # Containerd default configuration file
â”‚   â”œâ”€â”€ general.yml # Tasks executed for all hosts, master and workers
â”‚   â”œâ”€â”€ master.yml # Tasks for cluster init
â”‚   â”œâ”€â”€ output/ # Playbook output folder: will contains `admin.conf`, `kubeadm-join.sh`
â”‚   â””â”€â”€ worker.yml # Tasks to allow worker join the cluster
â”œâ”€â”€ README.md
â”œâ”€â”€ terraform.tf # Kubernetes Provision Terraform file
â””â”€â”€ Vagrantfile  # Vagrantfile with Virtualbox provider, Ansible provisioner
```

## Considerations
Regarding the default [microservice app](https://github.com/GoogleCloudPlatform/microservices-demo) Helm Chart, the `frontend` Deployment and Service definitions has been slightly modified by:
- Changing from `LoadBalancer` to `NodePort` as the local deploy of K8s cluster will not support the `LoadBalancer` service type.
- Setted the number of `replicas` to $3$, setting the `RollingUpdate` strategy with `maxUnavailable: 0` to ensure that all existing pods remained active during the update and `maxSurge: 2` so at most two extra pods can be created during the update process. 

Further improvements, considered but not implemented, are to use `readinessProbe` before route traffic to new pod and more check on `resources` to ensure faster allocation.



## References & sources
<a name="sources"></a>

1. [Kubernetes Blog - Kubernetes Setup Using Ansible and Vagrant](https://kubernetes.io/blog/2019/03/15/kubernetes-setup-using-ansible-and-vagrant/)
2. [Kubernetes k8s.io repository article](https://medium.com/@martin.hodges/installing-kubernetes-from-the-new-k8s-io-repository-using-ansible-8e7319fa97fd)
3. [Configure systemd cgroup driver](https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd-systemd)
4. [Container Runtimes - Forwarding IPv4 and letting iptables see bridged traffic](https://v1-29.docs.kubernetes.io/docs/setup/production-environment/container-runtimes/#forwarding-ipv4-and-letting-iptables-see-bridged-traffic)
5. [Terraform - Kubernetes Provider | Example Usage](https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs?ajs_aid=9178b4d2-7acb-417d-9ef2-a70cdc36042b&product_intent=terraform&utm_source=WEBSITE&utm_medium=WEB_IO&utm_offer=ARTICLE_PAGE&utm_content=DOCS#example-usage)
6. [Deploy application on K8s with Helm | Medium](https://wkrzywiec.medium.com/how-to-deploy-application-on-kubernetes-with-helm-39f545ad33b8)
6. *A couple of StackOverflow threads ğŸ˜*